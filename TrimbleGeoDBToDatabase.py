# Written by Scott D. Miller
# National Park Service, Arctic Inventory and Monitoring programs
# Shallow Lakes Monitoring Protocol
# https://www.nps.gov/im/cakn/shallowlakes.htm
# Purpose: Import field data into the lakes monitoring database. This python script translates the field data in the geodatabase 
# generated by the Shallow Lakes monitoring Trimble field computer
# into a series of SQL insert scripts that can be executed against the lakes monitoring database.
# U.S. Government Public Domain License



# Import utilities
import getpass
import datetime  
import os
    
# Input parameters
GeoDB = arcpy.GetParameterAsText(0) # ArcTool parameter # 1: The input geodatabase (Workspace).

# set the workspace
arcpy.env.workspace = GeoDB

# Get the name of the geodatabase from the full path of the source geodatabase
SourceFilename = os.path.basename(GeoDB) # Gets the filename from the path from above.

# Get the username
username = getpass.getuser()

# Standard header information to put in each sql script
HeaderInfo = "-- NPS Arctic and Central Alaska Inventory and Monitoring Program, Shallow Lakes Monitoring\n"
HeaderInfo = HeaderInfo + "-- Field data import script generated by " + username + " on " + str(datetime.datetime.now())  + ".\n"
HeaderInfo = HeaderInfo + "-- Source geodatabase: " + GeoDB + "\n\n"
HeaderInfo = HeaderInfo + "USE AK_ShallowLakes\nBEGIN TRANSACTION -- COMMIT  ROLLBACK You must either commit or rollback the insert queries in this transaction or the database will be left in a hanging state.\n"

# Translates the data in the Secchi_Joined featureclass into a script of SQL insert queries that can be executed on the AK_ShallowLakes database.
# NOTE: secchi depth is stored in the tblEvents table so this script tries to create a new event. There is no Secchi depth table in the database.
# Since events must be present to insert the other records this script must be run first.
# This may not be what you expect.
def Export_Secchi_Joined():
    try:
        FeatureClass = "Secchi_Joined"

        # This will be the output SQL file, same directory as the source geodatabase.
        SqlFile = os.path.dirname(arcpy.env.workspace) + '/' + SourceFilename + '_RunFirst_' + FeatureClass + '_Insert.sql'
        
        # if the CSV file exists already then delete it 
        if os.path.exists(SqlFile):
            arcpy.AddMessage("File exists: " + SqlFile + '. Deleted')
            os.remove(SqlFile)
        SqlFile = open(SqlFile,'a')  

        SqlFile.write("-- NOTE: Running this script will create new sampling events in the tblEvents table. The lakes referenced in this script must exist in the tblPonds table prior to running this script. Secchi depth data is stored in tblEvents. Rollback and correct any problems on error, then run again.\n")

        # Create the first half of the SQL insert query
        SqlPrefix = 'INSERT INTO tblEvents(PONDNAME,SAMPLEDATE,SECCHIDEPTH,SECCHIONBOTTOM,SECCHINOTES) VALUES('
        
        # Get the featureclass' field names
        Fields = arcpy.ListFields(FeatureClass)
        Field_names =  [Field.name for Field in Fields]



        # Validation queries
        LakeExistQueries = "-- Validate the lake exists in tblPonds\nSELECT\n"
        EventExistsQueries = "-- Validate the sampling event exists in tblEvents\n"

        # Insert queries
        InsertQueries = ""


        # loop through the data rows and translate the data cells into an SQL insert query
        for row in arcpy.da.SearchCursor(FeatureClass,Field_names):
            i = 0

            # Get the field values into variables
            PondName = str(row[11])
            SampleDate = str(row[8])
            SECCHIDEPTH = str(row[5])
            if row[6] == "Yes":
                SECCHIONBOTTOM = '1' ''
            else: 
                SECCHIONBOTTOM = '0'
            SECCHINOTES = str(row[7])
            
            
            SampleDateDate = datetime.datetime.strptime(SampleDate, '%Y-%m-%d %H:%M:%S')
            SampleDateShort = str(SampleDateDate.year) + "-" + str(SampleDateDate.month) + "-" + str(SampleDateDate.day)

            # Validate that the lake exists
            LakeExistQueries = LakeExistQueries + "(SELECT PondName FROM tblPonds WHERE Pondname = '" + PondName + "') As [" + PondName + "],\n"

            # Validate that the sampling event exists
            EventExistsQueries = EventExistsQueries  + "SELECT Pondname,SampleDate FROM tblEvents WHERE Pondname = '" + PondName + "' And SampleDate = '" + SampleDateShort + "'\n"

            # Write the insert query to file
            InsertQueries = InsertQueries + "-- " + SqlPrefix + "'" + PondName + "','" + SampleDateShort + "'," + SECCHIDEPTH + "," + SECCHIONBOTTOM + ",'" + SECCHINOTES + "');\n"

            # Append the lake name to LakesList
            #LakesList = LakesList + "'" + PondName + "',"

        # Write the header info to file
        #SqlFile.write(HeaderInfo)
        
        SqlFile.write("-- Validate the ponds exist\n" + LakeExistQueries.rstrip().rstrip(',') + "\n\n")
        SqlFile.write("-- Insert queries\n" + EventExistsQueries + "\n\n")
        SqlFile.write("Insert queries\n-- BEGIN TRANSACTION -- COMMIT ROLLBACK\n" + InsertQueries + "\n\n")
        # Write a utility SELECT query to make it easy to validate that the lakes exist in tblPonds
        #SqlFile.write("\n-- Utility query to validate that the lakes exist in tblPonds\n")
        #SqlFile.write("-- SELECT PondName FROM tblPonds WHERE PondName in (" + LakesList.rstrip(',') + ") ORDER BY PondName\n\n")

        # Let user know we're done
        FinishedMessage = FeatureClass + " data written to " + SqlFile.name.replace("/","\\"+ "\n")
        print(FinishedMessage)
        arcpy.AddMessage(FinishedMessage)

    # When something goes wrong, let user know
    except Exception as e:
        error = 'Error: ' + FeatureClass + ' ' + str(e)
        arcpy.AddMessage(error)
        print(error)


# Translates the data in the Depth_Joined featureclass into a script of SQL insert queries that can be executed on the AK_ShallowLakes database.
def Export_Depth_Joined():
    try:
        FeatureClass = "Depth_Joined"

        # This will be the output SQL file, same directory as the source geodatabase.
        SqlFile = os.path.dirname(arcpy.env.workspace) + '/' + SourceFilename + '_' + FeatureClass + '_Insert.sql'
        
        # if the CSV file exists already then delete it 
        if os.path.exists(SqlFile):
            arcpy.AddMessage("File exists: " + SqlFile + '. Deleted')
            os.remove(SqlFile)
        SqlFile = open(SqlFile,'a')  

        # Create the first half of the SQL insert query
        SqlPrefix = 'INSERT INTO tblPondDepths(PONDNAME,SAMPLEDATE,GPS_TIME,LATITUDE,LONGITUDE,DEPTH,COMMENTS_DEPTHS,DATAFILE,SOURCE) VALUES('
        
        # Get the featureclass' field names
        Fields = arcpy.ListFields(FeatureClass)
        Field_names =  [Field.name for Field in Fields]

        # Write the header info to file
        SqlFile.write(HeaderInfo)

        # loop through the data rows and translate the data cells into an SQL insert query
        for row in arcpy.da.SearchCursor(FeatureClass,Field_names):
            i = 0

            # Get the field values into variables
            PondName = str(row[10])
            SampleDate = str(row[7])
            GPS_Time = str(row[7])
            Latitude = str(row[9])
            Longitude = str(row[8])
            Depth = str(row[4])
            Comments_Depths = str(row[5])
            DataFile = SourceFilename
            Source = SourceFilename
            #SampleDateShort = datetime.strptime(SampleDate, '%m/%d/%y')

            # Write the insert query to file
            SqlFile.write(SqlPrefix + "'" + PondName + "','" + SampleDate + "','" + GPS_Time + "'," + Latitude + "," + Longitude + "," + Depth + ",'" + Comments_Depths + "','" + DataFile + "','" + Source + "');\n")

        # Write convenience commit/rollback options    
        SqlFile.write("\n-- COMMIT\n-- ROLLBACK")

        # Let user know we're done
        FinishedMessage = FeatureClass + " data written to " + SqlFile.name.replace("/","\\" + "\n")
        print(FinishedMessage)
        arcpy.AddMessage(FinishedMessage)

    # When something goes wrong, let user know
    except expression as identifier:
        error = 'Error: ' + FeatureClass + ' ' + str(e)
        arcpy.AddMessage(error)
        print(error)

# Translates the data in the Loons_Joined featureclass into a script of SQL insert queries that can be executed on the AK_ShallowLakes database.
# NOTE: secchi depth is stored in the tblEvents table so this script tries to create a new event. There is no Secchi depth table in the database.
# Since events must be present to insert the other records this script must be run first.
# This may not be what you expect.
def Export_Loons_Joined():
    try:
        FeatureClass = "Loons_Joined"

        # This will be the output SQL file, same directory as the source geodatabase.
        SqlFile = os.path.dirname(arcpy.env.workspace) + '/' + SourceFilename + '_' + FeatureClass + '_Insert.sql'
        
        # if the CSV file exists already then delete it 
        if os.path.exists(SqlFile):
            arcpy.AddMessage("File exists: " + SqlFile + '. Deleted')
            os.remove(SqlFile)
        SqlFile = open(SqlFile,'a')  

        # Create the first half of the SQL insert query
        SqlPrefix = 'INSERT INTO tblEvents(PONDNAME,SAMPLEDATE,SPECIES,NUM_ADULTS,NUM_YOUNG,DETECTION_TYPE,LATITUDE,LONGITUDE,COMMENTS,SOURCE) VALUES('
        
        # Get the featureclass' field names
        Fields = arcpy.ListFields(FeatureClass)
        Field_names =  [Field.name for Field in Fields]

        # Write the header info to file
        SqlFile.write(HeaderInfo)

        # loop through the data rows and translate the data cells into an SQL insert query
        for row in arcpy.da.SearchCursor(FeatureClass,Field_names):
            i = 0

            # Get the field values into variables
            PondName = str(row[13])
            SampleDate = str(row[10])
            # OBSERVER = str(row[8])
            SPECIES = str(row[4])
            NUM_ADULTS = str(row[6])
            NUM_YOUNG = str(row[7])
            DETECTION_TYPE = str(row[5])
            LATITUDE = str(row[12])
            LONGITUDE = str(row[11])
            # VEG_TYPE = str(row[8])
            COMMENTS = str(row[9])
            SOURCE = SourceFilename
            #SampleDateShort = datetime.strptime(SampleDate, '%m/%d/%y')

            # Write the insert query to file
            SqlFile.write(SqlPrefix + "'" + PondName + "','" + SampleDate + "','" + SPECIES + "'," + NUM_ADULTS + "," + NUM_YOUNG + ",'" + DETECTION_TYPE + "'," + LATITUDE + "," + LONGITUDE + ",'" + COMMENTS + "','" + SOURCE + "');\n")

        # Write convenience commit/rollback options    
        SqlFile.write("\n-- COMMIT\n-- ROLLBACK")

        # Let user know we're done
        FinishedMessage = FeatureClass + " data written to " + SqlFile.name.replace("/","\\"+ "\n")
        print(FinishedMessage)
        arcpy.AddMessage(FinishedMessage)

    # When something goes wrong, let user know
    except e:
        error = 'Error: ' + FeatureClass + ' ' + str(e)
        arcpy.AddMessage(error)
        print(error)

# start here
Export_Secchi_Joined()
Export_Depth_Joined()
Export_Loons_Joined()