# Written by Scott D. Miller
# National Park Service, Arctic Inventory and Monitoring programs
# Shallow Lakes Monitoring Protocol
# https://www.nps.gov/im/cakn/shallowlakes.htm
# Purpose: Import field data into the lakes monitoring database. This python script translates the field data in the geodatabase 
# generated by the Shallow Lakes monitoring Trimble field computer
# into a series of SQL insert scripts that can be executed against the lakes monitoring database.

# Import utilities
import getpass
import datetime  
    
# Input parameters
# GeoDB = arcpy.GetParameterAsText(0) # The input geodatabase (Workspace)
GeoDB = "C:/Work/VitalSigns/ARCN-CAKN Shallow Lakes/Local/2020-07 Geodatabase/2020/YUCH_2020/YUCH_2020_Deployment.gdb"

# set the workspace
arcpy.env.workspace = GeoDB

# Get the name of the geodatabase from the full path of the source geodatabase
SourceFilename = os.path.basename(GeoDB) # Gets the filename from the path from above.

# Get the username
username = getpass.getuser()

# Standard header information to put in each sql script
HeaderInfo = "-- NPS Arctic and Central Alaska Inventory and Monitoring Program, Shallow Lakes Monitoring\n-- Field data import script generated by " + username + " on " + str(datetime.datetime.now())  + ".\n-- Source geodatabase: " + GeoDB + "\n\nUSE AK_ShallowLakes\nBEGIN TRANSACTION -- COMMIT  ROLLBACK\n"

# Translates the data in the Depth_Joined featureclass into a script of SQL insert queries that can be executed on the AK_ShallowLakes database.
def Export_Depth_Joined():
    try:
        FeatureClass = "Depth_Joined"

        # This will be the output SQL file, same directory as the source geodatabase.
        CSVFile = os.path.dirname(arcpy.env.workspace) + '/' + FeatureClass + '_Insert.sql'
        
        # if the CSV file exists already then delete it 
        if os.path.exists(CSVFile):
            arcpy.AddMessage("File exists: " + CSVFile + '. Deleted')
            os.remove(CSVFile)
        CSVFile = open(CSVFile,'a')  

        # Create the first half of the SQL insert query
        SqlPrefix = 'INSERT INTO tblPondDepths(PONDNAME,SAMPLEDATE,GPS_TIME,LATITUDE,LONGITUDE,DEPTH,COMMENTS_DEPTHS,DATAFILE,SOURCE) VALUES('
        
        # Get the featureclass' field names
        Fields = arcpy.ListFields(FeatureClass)
        Field_names =  [Field.name for Field in Fields]

        # Write the header info to file
        CSVFile.write(HeaderInfo)

        # loop through the data rows and translate the data cells into an SQL insert query
        for row in arcpy.da.SearchCursor(FeatureClass,Field_names):
            i = 0

            # Get the field values into variables
            PondName = str(row[10])
            SampleDate = str(row[7])
            GPS_Time = str(row[7])
            Latitude = str(row[9])
            Longitude = str(row[8])
            Depth = str(row[4])
            Comments_Depths = str(row[5])
            DataFile = SourceFilename
            Source = SourceFilename

            # Write the insert query to file
            CSVFile.write(SqlPrefix + "'" + PondName + "','" + SampleDate + "','" + GPS_Time + "'," + Latitude + "," + Longitude + "," + Depth + ",'" + Comments_Depths + "','" + DataFile + "','" + Source + "');\n")

        # Write convenience commit/rollback options    
        CSVFile.write("\n-- COMMIT\n-- ROLLBACK")

        # Let user know we're done
        FinishedMessage = FeatureClass + " data written to " + CSVFile.name.replace("/","\\")
        print(FinishedMessage)
        arcpy.AddMessage(FinishedMessage)

    # When something goes wrong, let user know
    except expression as identifier:
        error = 'Error: ' + FeatureClass + ' ' + str(e)
        arcpy.AddMessage(error)
        print(error)

# start here
Export_Depth_Joined()